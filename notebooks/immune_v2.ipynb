{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a50548-b81f-44c6-8946-f7202062fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.14 (main, Oct 21 2025, 18:27:30) [Clang 20.1.8 ]\n"
     ]
    }
   ],
   "source": [
    "# Run this first in a separate cell\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c976be03-4c75-4d78-8a89-eeb1dd9e3d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED SELF-HEALING AI CYBER IMMUNE NETWORK\n",
      "================================================================================\n",
      "TensorFlow version: 2.16.2\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import time\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ADVANCED SELF-HEALING AI CYBER IMMUNE NETWORK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9267acd2-7439-4937-abbe-05dacf10fc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 2: Loading Dataset\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total dataset shape: (58596, 57)\n",
      "Columns: 57\n",
      "\n",
      "First few rows:\n",
      "  Category  pslist.nproc  pslist.nppid  pslist.avg_threads  \\\n",
      "0   Benign            45            17           10.555556   \n",
      "1   Benign            47            19           11.531915   \n",
      "2   Benign            40            14           14.725000   \n",
      "\n",
      "   pslist.nprocs64bit  pslist.avg_handlers  dlllist.ndlls  \\\n",
      "0                   0           202.844444           1694   \n",
      "1                   0           242.234043           2074   \n",
      "2                   0           288.225000           1932   \n",
      "\n",
      "   dlllist.avg_dlls_per_proc  handles.nhandles  handles.avg_handles_per_proc  \\\n",
      "0                   38.50000              9129                    212.302326   \n",
      "1                   44.12766             11385                    242.234043   \n",
      "2                   48.30000             11529                    288.225000   \n",
      "\n",
      "   ...  svcscan.kernel_drivers  svcscan.fs_drivers  svcscan.process_services  \\\n",
      "0  ...                     221                  26                        24   \n",
      "1  ...                     222                  26                        24   \n",
      "2  ...                     222                  26                        27   \n",
      "\n",
      "   svcscan.shared_process_services  svcscan.interactive_process_services  \\\n",
      "0                              116                                     0   \n",
      "1                              118                                     0   \n",
      "2                              118                                     0   \n",
      "\n",
      "   svcscan.nactive  callbacks.ncallbacks  callbacks.nanonymous  \\\n",
      "0              121                    87                     0   \n",
      "1              122                    87                     0   \n",
      "2              120                    88                     0   \n",
      "\n",
      "   callbacks.ngeneric   Class  \n",
      "0                   8  Benign  \n",
      "1                   8  Benign  \n",
      "2                   8  Benign  \n",
      "\n",
      "[3 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 2: Loading Dataset\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df = pd.read_csv('../data/csvs/Obfuscated-MalMem2022.csv')\n",
    "\n",
    "print(f\"\\nTotal dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee56dd5-3f07-4c07-9421-a2f6e913cc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3: Data Cleaning and Preprocessing\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Dataset Info:\n",
      "Total rows: 58596\n",
      "Total columns: 57\n",
      "\n",
      "Column names:\n",
      "['Category', 'pslist.nproc', 'pslist.nppid', 'pslist.avg_threads', 'pslist.nprocs64bit', 'pslist.avg_handlers', 'dlllist.ndlls', 'dlllist.avg_dlls_per_proc', 'handles.nhandles', 'handles.avg_handles_per_proc', 'handles.nport', 'handles.nfile', 'handles.nevent', 'handles.ndesktop', 'handles.nkey', 'handles.nthread', 'handles.ndirectory', 'handles.nsemaphore', 'handles.ntimer', 'handles.nsection', 'handles.nmutant', 'ldrmodules.not_in_load', 'ldrmodules.not_in_init', 'ldrmodules.not_in_mem', 'ldrmodules.not_in_load_avg', 'ldrmodules.not_in_init_avg', 'ldrmodules.not_in_mem_avg', 'malfind.ninjections', 'malfind.commitCharge', 'malfind.protection', 'malfind.uniqueInjections', 'psxview.not_in_pslist', 'psxview.not_in_eprocess_pool', 'psxview.not_in_ethread_pool', 'psxview.not_in_pspcid_list', 'psxview.not_in_csrss_handles', 'psxview.not_in_session', 'psxview.not_in_deskthrd', 'psxview.not_in_pslist_false_avg', 'psxview.not_in_eprocess_pool_false_avg', 'psxview.not_in_ethread_pool_false_avg', 'psxview.not_in_pspcid_list_false_avg', 'psxview.not_in_csrss_handles_false_avg', 'psxview.not_in_session_false_avg', 'psxview.not_in_deskthrd_false_avg', 'modules.nmodules', 'svcscan.nservices', 'svcscan.kernel_drivers', 'svcscan.fs_drivers', 'svcscan.process_services', 'svcscan.shared_process_services', 'svcscan.interactive_process_services', 'svcscan.nactive', 'callbacks.ncallbacks', 'callbacks.nanonymous', 'callbacks.ngeneric', 'Class']\n",
      "\n",
      "Missing values per column:\n",
      "No missing values\n",
      "\n",
      "Data types:\n",
      "int64      40\n",
      "float64    15\n",
      "object      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Found label column: 'Class'\n",
      "\n",
      "Label distribution in 'Class':\n",
      "Class\n",
      "Benign     29298\n",
      "Malware    29298\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 3: Data Cleaning and Preprocessing\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values\")\n",
    "\n",
    "# Display data types\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Check if there's a label column\n",
    "label_candidates = ['Label', 'Class', 'Category', 'Filename']\n",
    "label_col = None\n",
    "for col in label_candidates:\n",
    "    if col in df.columns:\n",
    "        label_col = col\n",
    "        print(f\"\\nFound label column: '{label_col}'\")\n",
    "        break\n",
    "\n",
    "if label_col is None:\n",
    "    print(\"\\nWARNING: No standard label column found. Using last column as label.\")\n",
    "    label_col = df.columns[-1]\n",
    "\n",
    "# Display label distribution\n",
    "print(f\"\\nLabel distribution in '{label_col}':\")\n",
    "print(df[label_col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3ef02d-8900-4281-b8f7-c950d0eb42bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 4: Creating Binary Labels\n",
      "--------------------------------------------------------------------------------\n",
      "Binary label distribution:\n",
      "Label\n",
      "0    29298\n",
      "1    29298\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class 0 (Benign): 29298 samples\n",
      "Class 1 (Malicious): 29298 samples\n",
      "Class imbalance ratio: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 4: Creating Binary Labels\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create binary labels\n",
    "if df[label_col].dtype == 'object':\n",
    "    # If labels are strings (e.g., 'Benign', 'Malware', 'Spyware')\n",
    "    def create_binary_label(x):\n",
    "        x_str = str(x).lower()\n",
    "        if 'benign' in x_str:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    df['Label'] = df[label_col].apply(create_binary_label)\n",
    "else:\n",
    "    # If labels are already numeric\n",
    "    df['Label'] = df[label_col]\n",
    "\n",
    "print(\"Binary label distribution:\")\n",
    "print(df['Label'].value_counts())\n",
    "print(f\"\\nClass 0 (Benign): {(df['Label'] == 0).sum()} samples\")\n",
    "print(f\"Class 1 (Malicious): {(df['Label'] == 1).sum()} samples\")\n",
    "\n",
    "# Check for class imbalance\n",
    "class_ratio = df['Label'].value_counts()[1] / df['Label'].value_counts()[0]\n",
    "print(f\"Class imbalance ratio: {class_ratio:.2f}\")\n",
    "\n",
    "if df['Label'].value_counts().min() == 0:\n",
    "    print(\"\\nWARNING: One class has zero samples!\")\n",
    "    print(\"Please check your dataset and labeling logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3132ed5-5816-48e8-99a8-fcba39e5f9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 5: Feature Selection and Preparation\n",
      "--------------------------------------------------------------------------------\n",
      "Features shape: (58596, 55)\n",
      "Labels shape: (58596,)\n",
      "\n",
      "Handling missing and infinite values...\n",
      "\n",
      "Checking for constant features...\n",
      "Found 3 constant features, removing...\n",
      "\n",
      "Final feature matrix shape: (58596, 52)\n",
      "Total samples: 58596\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 5: Feature Selection and Preparation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Separate features and labels\n",
    "# Drop non-numeric columns\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Remove label from features if present\n",
    "feature_cols = [col for col in numeric_df.columns if col not in ['Label', 'Unnamed: 0', 'index']]\n",
    "X = numeric_df[feature_cols].values\n",
    "y = df['Label'].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Handle NaN and infinite values\n",
    "print(\"\\nHandling missing and infinite values...\")\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Check for constant features\n",
    "print(\"\\nChecking for constant features...\")\n",
    "constant_features = []\n",
    "for i in range(X.shape[1]):\n",
    "    if np.std(X[:, i]) == 0:\n",
    "        constant_features.append(i)\n",
    "\n",
    "if len(constant_features) > 0:\n",
    "    print(f\"Found {len(constant_features)} constant features, removing...\")\n",
    "    X = np.delete(X, constant_features, axis=1)\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")\n",
    "print(f\"Total samples: {len(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f97a4578-7b4a-4cd1-a91e-b67ad3bcb9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 6: Train-Test Split\n",
      "--------------------------------------------------------------------------------\n",
      "Training set: 46876 samples\n",
      "Test set: 11720 samples\n",
      "\n",
      "Training label distribution:\n",
      "  Class 0: 23438\n",
      "  Class 1: 23438\n",
      "\n",
      "Test label distribution:\n",
      "  Class 0: 5860\n",
      "  Class 1: 5860\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 6: Train-Test Split\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining label distribution:\")\n",
    "print(f\"  Class 0: {(y_train == 0).sum()}\")\n",
    "print(f\"  Class 1: {(y_train == 1).sum()}\")\n",
    "print(f\"\\nTest label distribution:\")\n",
    "print(f\"  Class 0: {(y_test == 0).sum()}\")\n",
    "print(f\"  Class 1: {(y_test == 1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b9f039-2d11-4cef-b02d-0f283a9f9d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 7: Advanced Feature Engineering\n",
      "--------------------------------------------------------------------------------\n",
      "Original features: 52\n",
      "Engineered features: 56\n",
      "Feature engineering complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 7: Advanced Feature Engineering\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class AdvancedFeatureEngineering:\n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()\n",
    "        \n",
    "    def engineer_features(self, X, fit=True):\n",
    "        \"\"\"Create advanced features from raw data\"\"\"\n",
    "        if fit:\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Statistical features\n",
    "        X_mean = np.mean(X_scaled, axis=1).reshape(-1, 1)\n",
    "        X_std = np.std(X_scaled, axis=1).reshape(-1, 1)\n",
    "        X_max = np.max(X_scaled, axis=1).reshape(-1, 1)\n",
    "        X_min = np.min(X_scaled, axis=1).reshape(-1, 1)\n",
    "        \n",
    "        # Combine all features\n",
    "        X_engineered = np.concatenate([X_scaled, X_mean, X_std, X_max, X_min], axis=1)\n",
    "        return X_engineered\n",
    "\n",
    "feature_engineer = AdvancedFeatureEngineering()\n",
    "X_train_eng = feature_engineer.engineer_features(X_train, fit=True)\n",
    "X_test_eng = feature_engineer.engineer_features(X_test, fit=False)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Engineered features: {X_train_eng.shape[1]}\")\n",
    "print(\"Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4681e330-dfbf-4fb1-80d1-99111b2cd0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 8: Building Advanced Neural Network\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 13:28:09.771648: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-10-24 13:28:09.771689: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-10-24 13:28:09.771703: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-10-24 13:28:09.771720: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-24 13:28:09.771733: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced neural network created\n",
      "Total parameters: 57,601\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 8: Building Advanced Neural Network\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def build_advanced_model(input_dim):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    x = keras.layers.Dense(256, activation='relu')(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "advanced_model = build_advanced_model(X_train_eng.shape[1])\n",
    "advanced_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"Advanced neural network created\")\n",
    "print(f\"Total parameters: {advanced_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e329d03-28aa-4cec-a5d9-c737f683198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 9: Building Wasserstein GAN\n",
      "--------------------------------------------------------------------------------\n",
      "Wasserstein GAN created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 9: Building Wasserstein GAN\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class WGAN_GP:\n",
    "    def __init__(self, data_dim, latent_dim=128):\n",
    "        self.data_dim = data_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "        \n",
    "    def build_generator(self):\n",
    "        return keras.Sequential([\n",
    "            keras.layers.Dense(256, activation='relu', input_dim=self.latent_dim),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(512, activation='relu'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(1024, activation='relu'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(self.data_dim, activation='tanh')\n",
    "        ])\n",
    "    \n",
    "    def build_critic(self):\n",
    "        return keras.Sequential([\n",
    "            keras.layers.Dense(1024, activation='relu', input_dim=self.data_dim),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(512, activation='relu'),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(256, activation='relu'),\n",
    "            keras.layers.Dense(1)\n",
    "        ])\n",
    "    \n",
    "    def generate_samples(self, n_samples):\n",
    "        noise = np.random.normal(0, 1, (n_samples, self.latent_dim))\n",
    "        return self.generator.predict(noise, verbose=0)\n",
    "\n",
    "wgan = WGAN_GP(X_train_eng.shape[1])\n",
    "print(\"Wasserstein GAN created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd681add-b30f-4d13-8f71-df9d11116454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 10: Building Ensemble Detection System\n",
      "--------------------------------------------------------------------------------\n",
      "Ensemble system initialized\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 10: Building Ensemble Detection System\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class EnsembleDetector:\n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'neural_net': None,\n",
    "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "            'gradient_boost': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "        }\n",
    "        self.weights = {'neural_net': 0.5, 'random_forest': 0.25, 'gradient_boost': 0.25}\n",
    "        \n",
    "    def fit(self, X, y, nn_model):\n",
    "        print(\"Training ensemble components...\")\n",
    "        self.models['neural_net'] = nn_model\n",
    "        print(\"  Training Random Forest...\")\n",
    "        self.models['random_forest'].fit(X, y)\n",
    "        print(\"  Training Gradient Boosting...\")\n",
    "        self.models['gradient_boost'].fit(X, y)\n",
    "        print(\"Ensemble training complete\")\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        nn_pred = self.models['neural_net'].predict(X, verbose=0).flatten()\n",
    "        rf_pred = self.models['random_forest'].predict_proba(X)[:, 1]\n",
    "        gb_pred = self.models['gradient_boost'].predict_proba(X)[:, 1]\n",
    "        \n",
    "        ensemble_pred = (\n",
    "            self.weights['neural_net'] * nn_pred +\n",
    "            self.weights['random_forest'] * rf_pred +\n",
    "            self.weights['gradient_boost'] * gb_pred\n",
    "        )\n",
    "        return ensemble_pred\n",
    "\n",
    "ensemble = EnsembleDetector()\n",
    "print(\"Ensemble system initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2e7cac-9933-4829-acc3-deb9cb636fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 11: Building Federated Learning System\n",
      "--------------------------------------------------------------------------------\n",
      "Federated system with 10 nodes created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 11: Building Federated Learning System\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class FederatedLearningSystem:\n",
    "    def __init__(self, num_nodes=10, privacy_budget=1.0):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.privacy_budget = privacy_budget\n",
    "        self.global_model = None\n",
    "        \n",
    "    def split_data(self, X, y):\n",
    "        node_data = []\n",
    "        samples_per_node = len(X) // self.num_nodes\n",
    "        for i in range(self.num_nodes):\n",
    "            start = i * samples_per_node\n",
    "            end = start + samples_per_node\n",
    "            node_data.append((X[start:end], y[start:end]))\n",
    "        return node_data\n",
    "    \n",
    "    def train_node(self, X_node, y_node, input_dim):\n",
    "        model = build_advanced_model(input_dim)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_node, y_node, epochs=5, batch_size=64, verbose=0)\n",
    "        return model\n",
    "    \n",
    "    def federated_averaging(self, models):\n",
    "        avg_weights = []\n",
    "        for weights_list in zip(*[m.get_weights() for m in models]):\n",
    "            noise = np.random.laplace(0, self.privacy_budget, weights_list[0].shape)\n",
    "            avg_weights.append(np.mean(weights_list, axis=0) + noise * 0.01)\n",
    "        return avg_weights\n",
    "\n",
    "fed_system = FederatedLearningSystem(num_nodes=10)\n",
    "print(f\"Federated system with {fed_system.num_nodes} nodes created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d48c81c-bbcb-4bb3-9a48-cf7e3edc88f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 12: Building Reinforcement Learning Agent\n",
      "--------------------------------------------------------------------------------\n",
      "RL agent initialized\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 12: Building Reinforcement Learning Agent\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class RLResponseAgent:\n",
    "    def __init__(self, state_dim, action_dim=4):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_table = np.zeros((100, action_dim))\n",
    "        self.epsilon = 0.1\n",
    "        self.learning_rate = 0.1\n",
    "        self.gamma = 0.95\n",
    "        \n",
    "    def choose_action(self, state_hash):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        return np.argmax(self.q_table[state_hash % 100])\n",
    "\n",
    "rl_agent = RLResponseAgent(state_dim=X_train_eng.shape[1])\n",
    "print(\"RL agent initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "380b88ba-c127-4d00-a4fb-6399e3e4e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 13: Building Self-Healing System\n",
      "--------------------------------------------------------------------------------\n",
      "Self-healing system initialized\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 13: Building Self-Healing System\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class AdvancedSelfHealingSystem:\n",
    "    def __init__(self, model, ensemble, wgan, rl_agent):\n",
    "        self.model = model\n",
    "        self.ensemble = ensemble\n",
    "        self.wgan = wgan\n",
    "        self.rl_agent = rl_agent\n",
    "        self.threat_history = deque(maxlen=1000)\n",
    "        self.healing_log = []\n",
    "        self.threat_threshold = 0.7\n",
    "        \n",
    "    def detect_threat(self, sample):\n",
    "        ensemble_pred = self.ensemble.predict_proba(sample.reshape(1, -1))[0]\n",
    "        \n",
    "        if ensemble_pred > self.threat_threshold:\n",
    "            threat_level = \"CRITICAL\" if ensemble_pred > 0.9 else \"HIGH\"\n",
    "            return True, ensemble_pred, threat_level\n",
    "        return False, ensemble_pred, \"SAFE\"\n",
    "    \n",
    "    def auto_heal(self, threat_data, threat_labels):\n",
    "        print(\"  Initiating self-healing...\")\n",
    "        synthetic_samples = self.wgan.generate_samples(100)\n",
    "        combined_X = np.vstack([threat_data, synthetic_samples])\n",
    "        combined_y = np.hstack([threat_labels, np.ones(100)])\n",
    "        self.model.fit(combined_X, combined_y, epochs=3, batch_size=64, verbose=0)\n",
    "        self.healing_log.append({'timestamp': time.time(), 'status': 'HEALED'})\n",
    "        print(\"  System healed\")\n",
    "    \n",
    "    def adaptive_response(self, threat_level):\n",
    "        actions = ['MONITOR', 'ISOLATE', 'BLOCK', 'HEAL']\n",
    "        state_hash = hash(threat_level) % 100\n",
    "        action_idx = self.rl_agent.choose_action(state_hash)\n",
    "        return actions[action_idx]\n",
    "\n",
    "print(\"Self-healing system initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "586975ef-8a6a-4d23-95b9-fa7d71c8dff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 14: Training All Models\n",
      "--------------------------------------------------------------------------------\n",
      "Training deep neural network...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 13:28:10.608998: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 21ms/step - accuracy: 0.9269 - auc: 0.9772 - loss: 0.2471 - val_accuracy: 0.9899 - val_auc: 0.9941 - val_loss: 0.1222\n",
      "Epoch 2/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 19ms/step - accuracy: 0.9886 - auc: 0.9946 - loss: 0.0703 - val_accuracy: 0.9929 - val_auc: 0.9970 - val_loss: 0.0847\n",
      "Epoch 3/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - accuracy: 0.9914 - auc: 0.9975 - loss: 0.0449 - val_accuracy: 0.9946 - val_auc: 0.9980 - val_loss: 0.0577\n",
      "Epoch 4/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - accuracy: 0.9923 - auc: 0.9975 - loss: 0.0401 - val_accuracy: 0.9959 - val_auc: 0.9985 - val_loss: 0.0479\n",
      "Epoch 5/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 19ms/step - accuracy: 0.9934 - auc: 0.9978 - loss: 0.0366 - val_accuracy: 0.9959 - val_auc: 0.9982 - val_loss: 0.0447\n",
      "Epoch 6/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 19ms/step - accuracy: 0.9939 - auc: 0.9978 - loss: 0.0347 - val_accuracy: 0.9963 - val_auc: 0.9986 - val_loss: 0.0403\n",
      "Epoch 7/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - accuracy: 0.9941 - auc: 0.9978 - loss: 0.0324 - val_accuracy: 0.9962 - val_auc: 0.9988 - val_loss: 0.0283\n",
      "Epoch 8/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - accuracy: 0.9942 - auc: 0.9976 - loss: 0.0360 - val_accuracy: 0.9964 - val_auc: 0.9991 - val_loss: 0.0399\n",
      "Epoch 9/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - accuracy: 0.9947 - auc: 0.9979 - loss: 0.0306 - val_accuracy: 0.9957 - val_auc: 0.9986 - val_loss: 0.0560\n",
      "Epoch 10/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - accuracy: 0.9932 - auc: 0.9974 - loss: 0.0374 - val_accuracy: 0.9958 - val_auc: 0.9989 - val_loss: 0.0538\n",
      "Epoch 11/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 19ms/step - accuracy: 0.9943 - auc: 0.9978 - loss: 0.0331 - val_accuracy: 0.9961 - val_auc: 0.9978 - val_loss: 0.0486\n",
      "Epoch 12/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 19ms/step - accuracy: 0.9943 - auc: 0.9979 - loss: 0.0322 - val_accuracy: 0.9964 - val_auc: 0.9981 - val_loss: 0.0370\n",
      "Epoch 13/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - accuracy: 0.9944 - auc: 0.9981 - loss: 0.0295 - val_accuracy: 0.9964 - val_auc: 0.9992 - val_loss: 0.0330\n",
      "Epoch 14/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - accuracy: 0.9949 - auc: 0.9977 - loss: 0.0333 - val_accuracy: 0.9963 - val_auc: 0.9976 - val_loss: 0.0466\n",
      "Epoch 15/15\n",
      "\u001B[1m293/293\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 19ms/step - accuracy: 0.9948 - auc: 0.9978 - loss: 0.0303 - val_accuracy: 0.9964 - val_auc: 0.9977 - val_loss: 0.0452\n",
      "\n",
      "Training ensemble...\n",
      "Training ensemble components...\n",
      "  Training Random Forest...\n",
      "  Training Gradient Boosting...\n",
      "Ensemble training complete\n",
      "\n",
      "Running federated learning...\n",
      "  Node 1/10...\n",
      "  Node 2/10...\n",
      "  Node 3/10...\n",
      "  Node 4/10...\n",
      "  Node 5/10...\n",
      "  Node 6/10...\n",
      "  Node 7/10...\n",
      "  Node 8/10...\n",
      "  Node 9/10...\n",
      "  Node 10/10...\n",
      "  Federated averaging...\n",
      "All models trained\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 14: Training All Models\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Train neural network\n",
    "print(\"Training deep neural network...\")\n",
    "history = advanced_model.fit(\n",
    "    X_train_eng, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train ensemble\n",
    "print(\"\\nTraining ensemble...\")\n",
    "ensemble.fit(X_train_eng, y_train, advanced_model)\n",
    "\n",
    "# Federated learning\n",
    "print(\"\\nRunning federated learning...\")\n",
    "node_data = fed_system.split_data(X_train_eng, y_train)\n",
    "node_models = []\n",
    "for i, (X_n, y_n) in enumerate(node_data):\n",
    "    print(f\"  Node {i+1}/{fed_system.num_nodes}...\")\n",
    "    model = fed_system.train_node(X_n, y_n, X_train_eng.shape[1])\n",
    "    node_models.append(model)\n",
    "\n",
    "print(\"  Federated averaging...\")\n",
    "global_weights = fed_system.federated_averaging(node_models)\n",
    "fed_system.global_model = build_advanced_model(X_train_eng.shape[1])\n",
    "fed_system.global_model.set_weights(global_weights)\n",
    "fed_system.global_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"All models trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba137a04-22ff-42e5-8237-930f7f442c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 15: Testing Self-Healing System\n",
      "--------------------------------------------------------------------------------\n",
      "  Threat 1: CRITICAL (99.93%) -> MONITOR\n",
      "  Threat 2: CRITICAL (100.00%) -> MONITOR\n",
      "\n",
      "30 threats detected, triggering self-heal...\n",
      "  Initiating self-healing...\n",
      "  System healed\n",
      "\n",
      "Simulation complete: 30 threats handled\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 15: Testing Self-Healing System\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "healing_system = AdvancedSelfHealingSystem(advanced_model, ensemble, wgan, rl_agent)\n",
    "\n",
    "threats_detected = 0\n",
    "for i in range(min(50, len(X_test_eng))):\n",
    "    sample = X_test_eng[i]\n",
    "    is_threat, confidence, level = healing_system.detect_threat(sample)\n",
    "    \n",
    "    if is_threat:\n",
    "        threats_detected += 1\n",
    "        action = healing_system.adaptive_response(level)\n",
    "        if i < 3:\n",
    "            print(f\"  Threat {threats_detected}: {level} ({confidence:.2%}) -> {action}\")\n",
    "\n",
    "if threats_detected > 5:\n",
    "    print(f\"\\n{threats_detected} threats detected, triggering self-heal...\")\n",
    "    healing_system.auto_heal(X_train_eng[:100], y_train[:100])\n",
    "\n",
    "print(f\"\\nSimulation complete: {threats_detected} threats handled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8b433f4-9c63-4608-b0c9-21362af2922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 16: Final Evaluation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Deep Neural Network:\n",
      "  Accuracy: 0.9868\n",
      "  AUC: 0.9980\n",
      "\n",
      "Ensemble System:\n",
      "  Accuracy: 0.9978\n",
      "  AUC: 1.0000\n",
      "\n",
      "Federated Model:\n",
      "  Accuracy: 0.5000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5860\n",
      "           1       1.00      1.00      1.00      5860\n",
      "\n",
      "    accuracy                           1.00     11720\n",
      "   macro avg       1.00      1.00      1.00     11720\n",
      "weighted avg       1.00      1.00      1.00     11720\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PROJECT COMPLETE\n",
      "================================================================================\n",
      "Neural Network Accuracy: 98.68%\n",
      "Ensemble Accuracy: 99.78%\n",
      "Federated Accuracy: 50.00%\n",
      "Threats Detected: 30\n",
      "Healing Operations: 1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 16: Final Evaluation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Neural Network\n",
    "loss, acc, auc = advanced_model.evaluate(X_test_eng, y_test, verbose=0)\n",
    "print(f\"\\nDeep Neural Network:\")\n",
    "print(f\"  Accuracy: {acc:.4f}\")\n",
    "print(f\"  AUC: {auc:.4f}\")\n",
    "\n",
    "# Ensemble\n",
    "ensemble_preds = ensemble.predict_proba(X_test_eng)\n",
    "ensemble_acc = np.mean((ensemble_preds > 0.5) == y_test)\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_preds)\n",
    "print(f\"\\nEnsemble System:\")\n",
    "print(f\"  Accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"  AUC: {ensemble_auc:.4f}\")\n",
    "\n",
    "# Federated\n",
    "fed_loss, fed_acc = fed_system.global_model.evaluate(X_test_eng, y_test, verbose=0)\n",
    "print(f\"\\nFederated Model:\")\n",
    "print(f\"  Accuracy: {fed_acc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "y_pred = (ensemble_preds > 0.5).astype(int)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROJECT COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Neural Network Accuracy: {acc:.2%}\")\n",
    "print(f\"Ensemble Accuracy: {ensemble_acc:.2%}\")\n",
    "print(f\"Federated Accuracy: {fed_acc:.2%}\")\n",
    "print(f\"Threats Detected: {threats_detected}\")\n",
    "print(f\"Healing Operations: {len(healing_system.healing_log)}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88edff20-4307-4644-a425-4ed780afdb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING TRAINED MODELS FOR DASHBOARD\n",
      "================================================================================\n",
      "\n",
      "Saving advanced_model...\n",
      "  ✅ Saved as advanced_model.keras\n",
      "\n",
      "Saving ensemble...\n",
      "  ✅ Saved as ensemble.pkl\n",
      "\n",
      "Saving federated_model...\n",
      "  ✅ Saved as federated_model.keras\n",
      "\n",
      "Saving feature_engineer...\n",
      "  ✅ Saved as feature_engineer.pkl\n",
      "\n",
      "Saving model_metadata...\n",
      "  ✅ Saved as model_metadata.pkl\n",
      "\n",
      "================================================================================\n",
      "MODEL SAVING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Verifying saved files:\n",
      "  ✅ advanced_model.keras (722.63 KB)\n",
      "  ✅ ensemble.pkl (639.95 KB)\n",
      "  ✅ federated_model.keras (269.85 KB)\n",
      "  ✅ feature_engineer.pkl (1.22 KB)\n",
      "  ✅ model_metadata.pkl (0.17 KB)\n",
      "\n",
      "================================================================================\n",
      "You can now reload the dashboard!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING TRAINED MODELS FOR DASHBOARD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save Advanced Neural Network (modern Keras format)\n",
    "print(\"\\nSaving advanced_model...\")\n",
    "try:\n",
    "    advanced_model.save('advanced_model.keras')\n",
    "    print(\"  ✅ Saved as advanced_model.keras\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "# Save Ensemble System\n",
    "print(\"\\nSaving ensemble...\")\n",
    "try:\n",
    "    ensemble_data = {\n",
    "        'weights': ensemble.weights,\n",
    "        'random_forest': ensemble.models['random_forest'],\n",
    "        'gradient_boost': ensemble.models['gradient_boost']\n",
    "    }\n",
    "    with open('../models/ensemble.pkl', 'wb') as f:\n",
    "        pickle.dump(ensemble_data, f)\n",
    "    print(\"  ✅ Saved as ensemble.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "# Save Federated Model (modern Keras format)\n",
    "print(\"\\nSaving federated_model...\")\n",
    "try:\n",
    "    fed_system.global_model.save('federated_model.keras')\n",
    "    print(\"  ✅ Saved as federated_model.keras\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "# Save Feature Engineer (CORRECTED - save just the scaler)\n",
    "print(\"\\nSaving feature_engineer...\")\n",
    "try:\n",
    "    # Save just the scaler component (which is pickleable)\n",
    "    feature_engineer_data = {\n",
    "        'scaler': feature_engineer.scaler,\n",
    "        'type': 'RobustScaler'\n",
    "    }\n",
    "    with open('../models/feature_engineer.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_engineer_data, f)\n",
    "    print(\"  ✅ Saved as feature_engineer.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Error: {e}\")\n",
    "    print(f\"     Details: {str(e)}\")\n",
    "\n",
    "# Save model metadata\n",
    "print(\"\\nSaving model_metadata...\")\n",
    "try:\n",
    "    metadata = {\n",
    "        'accuracy': float(acc),\n",
    "        'ensemble_accuracy': float(ensemble_acc),\n",
    "        'federated_accuracy': float(fed_acc),\n",
    "        'threat_threshold': 0.7,\n",
    "        'num_features': int(X_train_eng.shape[1]),\n",
    "        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    with open('../models/model_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(\"  ✅ Saved as model_metadata.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL SAVING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify files were created\n",
    "print(\"\\nVerifying saved files:\")\n",
    "files_to_check = [\n",
    "    'advanced_model.keras',\n",
    "    'ensemble.pkl',\n",
    "    'federated_model.keras',\n",
    "    'feature_engineer.pkl',\n",
    "    'model_metadata.pkl'\n",
    "]\n",
    "\n",
    "for filename in files_to_check:\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename) / 1024  # KB\n",
    "        print(f\"  ✅ {filename} ({size:.2f} KB)\")\n",
    "    else:\n",
    "        print(f\"  ❌ {filename} - NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"You can now reload the dashboard!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad3534a5-9d3b-4abe-a528-39382260de4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ACTUAL PROJECT METRICS\n",
      "================================================================================\n",
      "Neural Network Accuracy: 0.9868\n",
      "Ensemble Accuracy: 0.9978\n",
      "Federated Learning Accuracy: 0.5000\n",
      "Number of Features: 56\n",
      "Training Date: 2025-10-24 14:13:36\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load metadata\n",
    "with open('../models/model_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ACTUAL PROJECT METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Neural Network Accuracy: {metadata['accuracy']:.4f}\")\n",
    "print(f\"Ensemble Accuracy: {metadata['ensemble_accuracy']:.4f}\")\n",
    "print(f\"Federated Learning Accuracy: {metadata['federated_accuracy']:.4f}\")\n",
    "print(f\"Number of Features: {metadata['num_features']}\")\n",
    "print(f\"Training Date: {metadata['training_date']}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
